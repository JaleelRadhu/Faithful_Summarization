model:
  name: meta-llama/Llama-3.1-8B-Instruct  #mistralai/Mistral-7B-Instruct-v0.3  #Qwen/Qwen2.5-7B-Instruct # #microsoft/Phi-3-mini-4k-instruct
  quantization: None
  backend: hf
  device: 0
  max_new_tokens: 512
  temperature: 0.7

data:
  base_summary_dir: /home/abdullahm/jaleel/Faithfullness_Improver/data
  results_dir: /home/abdullahm/jaleel/Faithfullness_Improver/results/
  output_dir: /home/abdullahm/jaleel/Faithfullness_Improver/outputs/

perspective_definitions_path: /home/abdullahm/jaleel/Faithfullness_Improver/prompts/perspective_defn.json

evaluator_prompt_path: /home/abdullahm/jaleel/Faithfullness_Improver/prompts/evaluator_prompt.txt
improver_prompt_path: /home/abdullahm/jaleel/Faithfullness_Improver/prompts/improver_prompt.txt

general_evaluator_prompt_path: /home/abdullahm/jaleel/Faithfullness_Improver/prompts/gen_eval_prompt.txt
general_improver_prompt_path: /home/abdullahm/jaleel/Faithfullness_Improver/prompts/gen_improver_prompt.txt



iteration:
  max_iterations: 5
  stopping_criterion: rouge-l-f
  tolerance: 0.01

# Network settings for vLLM requests
request_timeout: 300 # Timeout in seconds (e.g., 3 minutes)
max_retries: 3 # Number of times to retry on a timeout

is_improver_evaluator_model_same: True

improver_model:
  name: Qwen/Qwen2.5-7B-Instruct #mistralai/Mistral-7B-Instruct-v0.3 
  quantization: None
  backend: hf
  device: 0
  max_new_tokens: 512
  temperature: 0.7

evaluator_model:
  name:  mistralai/Mistral-7B-Instruct-v0.3 #Qwen/Qwen2.5-7B-Instruct
  quantization: None
  backend: hf
  device: 0
  max_new_tokens: 512
  temperature: 0.7